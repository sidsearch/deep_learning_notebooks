{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddharthbiswal/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np \n",
    "from scipy.stats import norm \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object that produces the true data distribution. this is the distribution that we will try and approximate with the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataDistribution(object):\n",
    "    def __init__(self):\n",
    "        self.mu = -1\n",
    "        self.sigma = 1\n",
    "        \n",
    "    def sample(self, N):\n",
    "        samples = np.random.normal(self.mu, self.sigma, N)\n",
    "        samples.sort()\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an object that produces the generator input noise distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GeneratorDistribution(object):\n",
    "    def __init__(self, range):\n",
    "        self.range = range\n",
    "        \n",
    "    def sample(self,N):\n",
    "        return np.linspace(-self.range, self.range, N)+\\\n",
    "            np.random.random(N) *0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the G, D need to be differentiable so that gradients can flow through \n",
    "the networks and we can train them using gradient descent. In the original GAN \n",
    "paper both networks were MLP, and so this is the network structure that we use here.\n",
    "Each MLP consists of 3 layers and uses tanh nonliearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp(input, h_dim):\n",
    "    init_cnst = tf.constant_initializer(0.0)\n",
    "    init_norm = tf.random_normal_initializer()\n",
    "    # initializes w0 \n",
    "    w0 = tf.get_variable('w0',[input.get_shape()[1],h_dim], initializer = init_norm)\n",
    "    b0 = tf.get_variable('b0',[h_dim],initializer =init_cnst)\n",
    "    w1 = tf.get_variable('w1',[h_dim,h_dim],initializer =init_norm)\n",
    "    b1 = tf.get_variable('b1',[h_dim],initializer = init_const)\n",
    "    h0 = tf.tanh(tf.matmul(input,w0) + b1)\n",
    "    h1 = tf.tanh(tf.matmul(h0,w1)+ b1)\n",
    "    return h1, [w0,b0,w1,b1]\n",
    "\n",
    "\n",
    "def generator(input, h_dim):\n",
    "    transform, params = mlp(input,h_dim)\n",
    "    init_const = tf.const_initializer(0.0)\n",
    "    init_norm  = tf.random_normal_initializer()\n",
    "    w = tf.get_variable('g_w',[h_dim,1],initializer = init_norm)\n",
    "    b = tf.get_variable('g_b',[1],initilizer = init_const)\n",
    "    h = tf.matmul(transform,w) + b\n",
    "    return h, params + [w,b]\n",
    "\n",
    "def discriminator(input, h_dim):\n",
    "    transform , params  = mlp(input, h_dim)\n",
    "    init_const = tf.constant_initializer(0.0)\n",
    "    init_norm  = tf.random_normal_initializer()\n",
    "    w = tf.get_variable('d_w',[h_dim,1],initializer = init_norm)\n",
    "    b = tf.get_variable('d_b',[1],initializer = init_const)\n",
    "    h = tf.sigmoid(tf.matmul(transform,w)+b)\n",
    "    return h, params + [w,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(loss, var_list, num_epochs):\n",
    "    initial_learning_rate = 0.01\n",
    "    decay = 0.95\n",
    "    num_decay_steps = num_epochs//4\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate =tf.train.exponential_decay(\n",
    "        initial_learnign_rate,\n",
    "        batch,\n",
    "        num_decay_steps,\n",
    "        decay,\n",
    "        staircase = True\n",
    "    )\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(\n",
    "        loss,\n",
    "        global_step = batch,\n",
    "        var_list = var_list\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#called every anim_frame_every epochs to capture a single snapshot of p(d), d\n",
    "# discriminator's boundary and p(g):\n",
    "\n",
    "def plot_distribution(GAN, session, loss_d, loss_g):\n",
    "    \n",
    "    # this function is to plot distributions\n",
    "    num_points = 100000\n",
    "    num_bins   = 100\n",
    "    xs = np.linspace(-GAN.gen.range, GAN.gen.range, num_points)\n",
    "    bins = np.linspace(-GAN.gen.range, GAN.gen.rage,num_bins)\n",
    "    \n",
    "    #p(data)\n",
    "    d_sample = np.zeros((num_points,1))\n",
    "#     for i in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self,data, gen, num_epochs):\n",
    "        self.data = data\n",
    "        self.gen  = gen\n",
    "        self.num_epochs = epochs\n",
    "        self.anim_frame_every = 100 \n",
    "        self.batch_size   = 128 \n",
    "        self.mlp_hidden_size = 4\n",
    "        self._create_model()\n",
    "        \n",
    "    def _create_model(self):\n",
    "        # in order to make sure that D is providing useful gradient info\n",
    "        # we pretrain D using a maximum likelihood objective, #\n",
    "        # we define the network for this pretraining step scoped as D_pre\n",
    "        \n",
    "        with tf.variable_scope('D_pre'):\n",
    "            self.pre_input = tf.placeholder(tf.float32,shape=(self.batch_size,1))\n",
    "            self.pre_labels = tf.placeholder(tf.float32,shape = (self.batch_size,1))\n",
    "            D, self_pre_theta = discriminator(self.pre_input,self.mlp_hidden_size)\n",
    "            self.pre_loss = tf.reduce_mean(tf.square(D-self.pre_labels))\n",
    "            self.pre_opt  = optimizer(self.pre_loss,None, self.num_epochs)\n",
    "            \n",
    "        # this defines the generator network- it takes samples from a noise \n",
    "        # distribution as input and passes them through an MLP\n",
    "        with tf.variable_scope('G'):\n",
    "            self.z = tf.placeholder(tf.float32, shape =(self.batch_size,1))\n",
    "            self.G, theta_g = generator(self.z, self.mlp_hidden_size)\n",
    "        \n",
    "        \n",
    "        # the D trie to tell the difference from samples from true distributions\n",
    "        # and passes them thru an MLP\n",
    "        \n",
    "        with tf.variable_scope('D') as scope:\n",
    "            self.x = tf.placeholder(tf.float32,shape = (self.batch_size,1))\n",
    "            self.D1, self.theta_d1 = discriminator(self.x, self.mlp_hidden_size)\n",
    "            scope.reuse_variables()\n",
    "            self.D2, self.theta_d2 = discriminator(self.G, self.mlp_hidden_size)\n",
    "            \n",
    "        # define the loss for discriminator and gen network a\n",
    "        self.loss_d = tf.reduce_mean(-tf.log(self.D1)-tf.log(1-self.D2))\n",
    "        self.loss_g = tf.reduce_mean(-tf.log(self.D2))\n",
    "        \n",
    "        self.opt_d  = optimizer(self.loss_d, self.theta_d2,self.num_epochs)\n",
    "        self.opt_g  = optimizer(self.loss_g, self.theta_g, self.num_epochs)\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            \n",
    "            # discriminator pre-training\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
